<!DOCTYPE HTML>
<html lang="zh-CN">
    <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="renderer" content="webkit">
    <meta name="HandheldFriendly" content="true">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="keywords" content="Maverick,熊猫小A,Galileo,blog" />
    <meta name="generator" content="Maverick 1.1" />
    <meta name="template" content="Galileo" />
    <link rel="alternate" type="application/rss+xml" title="智的博客 &raquo; RSS 2.0" href="/feed/index.xml" />
    <link rel="alternate" type="application/atom+xml" title="智的博客 &raquo; ATOM 1.0" href="/feed/atom/index.xml" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/AlanDecode/site-Blog@gh-pages/assets/galileo-1c8f2638f2.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/AlanDecode/site-Blog@gh-pages/assets/ExSearch/ExSearch-182e5a8868.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/AlanDecode/site-Blog@gh-pages/assets/katex.min.css">
    <link href="https://fonts.googleapis.com/css?family=Fira+Code&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Droid+Serif:400,700&display=swap">
    <script>
        var ExSearchConfig = {
            root: "",
            api: "https://cdn.jsdelivr.net/gh/AlanDecode/site-Blog@gh-pages/bb5fa62b4f89b103e03ee702ead78310.json"
        }
    </script>
    
<title>Let's talk about Clustering - 智的博客</title>
<meta name="author" content="熊猫小A" />
<meta name="description" content="A brief introduction on traditional clustering algorithms, and a novel new method —— Clustering by fast search and find of density peaks." />
<meta property="og:title" content="Let's talk about Clustering - 智的博客" />
<meta property="og:description" content="A brief introduction on traditional clustering algorithms, and a novel new method —— Clustering by fast search and find of density peaks." />
<meta property="og:site_name" content="智的博客" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/archives/320/" />
<meta property="og:image" content="" />
<meta property="article:published_time" content="2019-04-25T23:54:00-00.00" />
<meta name="twitter:title" content="Let's talk about Clustering - 智的博客" />
<meta name="twitter:description" content="A brief introduction on traditional clustering algorithms, and a novel new method —— Clustering by fast search and find of density peaks." />
<meta name="twitter:card" content="summary" />
<meta name="twitter:image" content="" />


    
<meta http-equiv="x-dns-prefetch-control" content="on">
<link rel="dns-prefetch" href="//cdn.jsdelivr.net" />
<link rel="dns-prefetch" href="//blog.imalan.cn" />
<!--
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/AlanDecode/site-Blog@gh-pages/brand_font/embed.css" />
<style>.brand{font-family:FZCuJinLFW,serif;font-weight: normal!important;}</style>
-->
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="black">
<link rel="apple-touch-icon" sizes="180x180" href="https://cdn.jsdelivr.net/gh/AlanDecode/site-Blog@gh-pages/apple-touch-icon.png?v=PY43YeeEKx">
<link rel="icon" type="image/png" sizes="32x32" href="https://cdn.jsdelivr.net/gh/AlanDecode/site-Blog@gh-pages/favicon-32x32.png?v=yyLyaqbyRG">
<link rel="icon" type="image/png" sizes="16x16" href="https://cdn.jsdelivr.net/gh/AlanDecode/site-Blog@gh-pages/favicon-16x16.png?v=yyLyaqbyRG">
<link rel="mask-icon" href="https://cdn.jsdelivr.net/gh/AlanDecode/site-Blog@gh-pages/safari-pinned-tab.svg?v=yyLyaqbyRG" color="#505050">
<link rel="shortcut icon" href="https://cdn.jsdelivr.net/gh/AlanDecode/site-Blog@gh-pages/favicon.ico?v=yyLyaqbyRG">
<meta name="application-name" content="三無計劃">
<meta name="apple-mobile-web-app-title" content="三無計劃">
<meta name="msapplication-TileColor" content="#000000">
<meta name="theme-color" content="#000000">
<meta name="baidu-site-verification" content="9BEwwo6Ibg" />

    </head>
    
    <body>
        
        <div class="container">
            <header id="ga-header">
                <div first>
                    <aside id="ga-brand">
                        <h1 class="brand"><a class="no-style" href="/">智的博客</a></h1>
                        <p>只坚持一种正义。我的正义。</p>
                    </aside>
                </div>
                <div second id="ga-nav">
                    <nav class="navs">
                        <ul><li><a class="ga-highlight" href="/" target="_self">首页</a></li><span class="separator">·</span><li><a class="ga-highlight" href="/archives/" target="_self">归档</a></li><span class="separator">·</span><li><a class="ga-highlight" href="/links/" target="_self">友链</a></li><span class="separator">·</span><li><a class="ga-highlight" href="/about/" target="_self">关于</a></li><span class="separator">·</span><li><a href="#" target="_self" class="search-form-input ga-highlight">搜索</a></li></ul>
                    </nav>
                </div>
            </header>
            <div class="wrapper">
                
<main>    
    <section class="ga-section ga-content">
        <article class="yue">
            <h1 class="ga-post_title">Let's talk about Clustering</h1>
            <span class="ga-post_meta ga-mono">
                <span>熊猫小A</span>
                <time>
                    2019-04-25
                </time>
                
                in <a no-style class="category" href="/category/偶尔Geek/">
                    偶尔Geek
                </a>
                
                
                <span class="leancloud_visitors" 
                    id="/archives/320/" 
                    data-flag-title="Let's talk about Clustering"> · <i class="leancloud-visitors-count"></i> Views</span>
                
            </span>
            <div class="ga-content_body">
                <p>This post is about clustering. I will make a short introduction of traditional methods, then talk about a magical algorithm published by Alex Rodriguez and Alessandro Laio on <a href="https://science.sciencemag.org/content/344/6191/1492.full">Science magazine</a>, 2014. I call it magical because it's simple yet powerful, and super intuitive. The principle behind it is really semantic.</p>
<p>Clustering, as a kind of unsupervised learning method, aims to divide dataset into some pieces (i.e. "classes"), making the <strong>intraclass difference</strong><sup id="fn_ref_1"><a href="#fn_1">1</a></sup> as small as possible while the <strong>interclass difference</strong><sup id="fn_ref_2"><a href="#fn_2">2</a></sup> as large as possible.</p>
<p>For example, the picture bellow shows 5000 points scattered on a 2-D plane, we humans can easily assign them into 15 groups, because there is an obvious distributing pattern of these points. The question is, how could computers finish this job?</p>
<p><figure style="flex: 50.0" ><img width="100" height="100" src="https://cdn.jsdelivr.net/gh/AlanDecode/site-Blog@gh-pages/archives/assets/54ba94e700b9b152723d423398fcef4d.png" /><figcaption>5000 points scattered on a 2-D plane</figcaption></figure></p>
<h2>Traditional methods</h2>
<p>In the past few decades many algorithms have been developed just to finish this "simple" task, some of them are quite delicate and beautiful. However, depending on the rationale behind them, they all have some shortcomings like any other algorithm does. They are either slow or too complicated to implement, or just cannot finish the job.</p>
<p>The <strong>C-Means</strong> algorithm might be the most famous one of its kind. We randomly choose some points as cluster centers at first, then we assign points into some clusters by their "distance" to cluster centers, then calculate new centers to replace old ones. This procedure is accomplished by repeating those steps several times, until cluster centers do not move anymore. This method is inspirational, and many similar methods have been developed, such as <strong>Fuzzy C-Means</strong>, which uses a fuzzy matrix to weight the target function. This kind of algorithms are easy to implement and are simple to understand, but as been mentioned above, the initial cluster centers are very important, and results vary under different initial state. That makes these algorithms not robust enough, thus not reliable in some cases.</p>
<p>Another kind of algorithms are build upon statistics, such as <strong>Guassian Mixed Model (GMM)</strong> algorithm. It assumes that data points are just samples from some mixed Guassian models, therefore the clustering task is equal to estimate several Guassian models, to make the posterior probability as large as possible. Theoretical foundation of these algorithms are clear, mathematics makes them look reliable. However, initial state still affects the result, and most importantly, the basic assumption — "data points are just samples from some mixed Guassian models" — isn't always true. Another problem is, this algorithm could be very slow with large datasets.</p>
<p>Some other algorithms exist, like <strong>Hierarchical Clustering</strong>, which works fine under discrete feature values. It regards each points as a single cluster at first, then combines two clusters each time, repeat this step until the total number of clusters reduces to a certain level. It sounds simple, but again, it's slow with large datasets, and not robust enough.</p>
<h2>A new method</h2>
<p>Algorithms mentioned above are highlights of clustering methods. Most of them are iterative methods, and could be time consuming with large datasets. People have found an acceptable principle: a good clustering method is an algorithm which can fully summarize the common features of points, while making different clusters faraway from each other.</p>
<p>A new method was introduced by Alex Rodriguez and Alessandro Laio in 2014, their paper was published on Science. This already indicates that their algorithm must have something special, since papers in this field are hardly seen on magazines like Science. It is worth mentioning that deep learning researches were already super popular in 2014, which makes this algorithm even more admirable — it has absolutely nothing to do with deep learning.</p>
<p>This algorithm is based on two principles, <strong>only two</strong>:</p>
<ol>
<li><strong>Cluster centers are points with high "Local Density ($\rho$)"</strong></li>
<li><strong>Cluster centers are points with high "Relative Distance ($\delta$)"</strong></li>
</ol>
<h3>Local Density</h3>
<p>How to understand these two principles? Well, the first is relatively easy: cluster centers tends to have more points around them, than other regular none-center points. Imagine a leader of a team, or the head of a gang, they are always surrounded by many other "little guys".</p>
<p>For each point, we simply count the number of points around it, that is, closer than a predefined cut-distance — $d_c$:</p>
<p>$$\rho_i = \sum \chi(d_{ij} - d_c)$$</p>
<p>And we have :</p>
<p>$$\chi(x) = 1|_{x<0}, \chi(x)=0|_{x\ge 0}$$</p>
<p>Geometrically, this step equals to drawing a circle (or spherical surface with higher dimensions) using $d_c$ as radius and $point_i$ as center, then count the number of points inside the circle.</p>
<h3>Relative Distance</h3>
<p>This is a little more difficult to understand. Actually "Relative distance" means "<strong>minimum distance to points with higher local density</strong>", that is for each point $P_i$ with local density $\rho_i$, we calculate distances between $P_i$ and any other point $P_j$ with local density $\rho_j > \rho_i$, then use the minimal distance as the "Relative Distance" of $P_i$. But WHY?</p>
<p>We can easily understand why we choose points with high Local Density ($\rho$) as cluster center candidates — because points around them are compact. But don't forget, another criteria is also important for cluster centers: <strong>distance between clusters should be as large as possible</strong>. It is reasonable, think about it: if cluster centers are too close to each other, there must be huge overlap between clusters, that usually isn't ideal result.</p>
<p>Again, imagine a man who want to be a leader of a team, firstly he gathered a lot "little guys", then he must want to keep distance to those "bigger bosses", otherwise he might lose in competition. The Relative Distance ($\delta$) is used to measure the distance between $P_i$ and any other cluster center candidates, as explained, this value should be as large as possible. Therefore, $\delta_i$ for $P_i$ could be calculated as:</p>
<p>$$\delta_i = \min(d_{ij}) |_{\rho_j>\rho_i}$$</p>
<p>If $P_i$ already has the biggest $\rho$, just let:</p>
<p>$$\delta_i = \max(\rho_j) |_{j\ne i}$$</p>
<p>And $d_{ij}$ means Euclidean distance between $P_i$ and $P_j$.</p>
<h3>Implementation</h3>
<p>I implemented this algorithm using Matlab, guess how many lines of code does it need?</p>
<div class="highlight"><pre><span></span><span class="k">function</span><span class="w"> </span>[centers_o, Rho_o, Delta_o] <span class="p">=</span><span class="w"> </span><span class="nf">fsfdp</span><span class="p">(</span>dataset_i, NumCategory_i, dc_i<span class="p">)</span><span class="w"></span>
<span class="w">    </span><span class="c">%%%%%</span>
    <span class="c">% Clustering by Fast search and find of density peaks</span>
    <span class="c">% AlanDecode</span>
    <span class="c">% </span>
    <span class="c">% dataset_i: N * D</span>
    <span class="c">% NumCategory_i: number of categories</span>
    <span class="c">% dc_i: cut distance</span>

    <span class="c">%% Dataset</span>
    <span class="n">dataset</span> <span class="p">=</span> <span class="n">dataset_i</span><span class="p">;</span>
    <span class="n">datasetSize</span> <span class="p">=</span> <span class="nb">size</span><span class="p">(</span><span class="n">dataset</span><span class="p">);</span>
    <span class="n">NumCategory</span> <span class="p">=</span> <span class="n">NumCategory_i</span><span class="p">;</span>

    <span class="c">%% Algorithm</span>

    <span class="c">% Calc distance between points</span>
    <span class="n">distMat</span> <span class="p">=</span> <span class="n">pdist</span><span class="p">(</span><span class="n">dataset</span><span class="p">);</span>
    <span class="n">distMat</span> <span class="p">=</span> <span class="n">squareform</span><span class="p">(</span><span class="n">distMat</span><span class="p">);</span>

    <span class="c">% Calc Rho (local density)</span>
    <span class="n">Rho</span> <span class="p">=</span> <span class="nb">zeros</span><span class="p">(</span><span class="n">datasetSize</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="mi">1</span><span class="p">);</span>
    <span class="n">for</span> <span class="s">idx</span> <span class="s">=</span> <span class="s">1:datasetSize(1)</span>
        <span class="n">dists</span> <span class="p">=</span> <span class="n">distMat</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="p">:);</span>
        <span class="n">Rho</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span> <span class="p">=</span> <span class="nb">size</span><span class="p">(</span><span class="n">dists</span><span class="p">(</span><span class="n">dists</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="o">&amp;</span> <span class="n">dists</span> <span class="o">&lt;</span> <span class="n">dc_i</span><span class="p">),</span> <span class="mi">2</span><span class="p">);</span>
    <span class="n">end</span>

    <span class="s">%</span> <span class="s">Sort</span>
    <span class="p">[</span><span class="o">~</span><span class="p">,</span> <span class="n">rhoIndex</span><span class="p">]</span> <span class="p">=</span> <span class="n">sort</span><span class="p">(</span><span class="n">Rho</span><span class="p">,</span> <span class="s">&#39;descend&#39;</span><span class="p">);</span>

    <span class="c">% Calc Delta (minimum distance between one point </span>
    <span class="c">% and any other point with higher density)</span>
    <span class="n">Delta</span> <span class="p">=</span> <span class="nb">zeros</span><span class="p">(</span><span class="n">datasetSize</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="mi">1</span><span class="p">);</span>
    <span class="n">for</span> <span class="s">idx</span> <span class="s">=</span> <span class="s">1:datasetSize(1)</span>
        <span class="n">if</span> <span class="s">idx</span> <span class="s">==</span> <span class="s">1</span>
            <span class="c">% point with highest rho</span>
            <span class="n">index</span> <span class="p">=</span> <span class="n">rhoIndex</span><span class="p">(</span><span class="n">idx</span><span class="p">);</span>

            <span class="c">% use highest distance as Delta</span>
            <span class="n">dists</span> <span class="p">=</span> <span class="n">distMat</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="p">:);</span>
            <span class="n">Delta</span><span class="p">(</span><span class="n">index</span><span class="p">)</span> <span class="p">=</span> <span class="n">max</span><span class="p">(</span><span class="n">dists</span><span class="p">);</span>
        <span class="n">else</span>
            <span class="s">%</span> <span class="s">current</span> <span class="s">point</span>
            <span class="n">index</span> <span class="p">=</span> <span class="n">rhoIndex</span><span class="p">(</span><span class="n">idx</span><span class="p">);</span>

            <span class="c">% points with higher rho</span>
            <span class="n">rhoIndexSlice</span> <span class="p">=</span> <span class="n">rhoIndex</span><span class="p">(</span><span class="mi">1</span><span class="p">:</span><span class="n">idx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">);</span>
            <span class="n">p1</span> <span class="p">=</span> <span class="n">dataset</span><span class="p">(</span><span class="n">rhoIndexSlice</span><span class="p">,</span> <span class="p">:);</span>

            <span class="c">% calc distance</span>
            <span class="n">p2</span> <span class="p">=</span> <span class="nb">repmat</span><span class="p">(</span><span class="n">dataset</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="p">:),</span> <span class="n">idx</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
            <span class="n">dists</span> <span class="p">=</span> <span class="nb">sqrt</span><span class="p">(</span><span class="n">sum</span><span class="p">((</span><span class="n">p1</span> <span class="o">-</span> <span class="n">p2</span><span class="p">)</span><span class="o">.^</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">));</span>

            <span class="c">% minimal distance</span>
            <span class="n">Delta</span><span class="p">(</span><span class="n">index</span><span class="p">)</span> <span class="p">=</span> <span class="n">min</span><span class="p">(</span><span class="n">dists</span><span class="p">);</span>
        <span class="n">end</span>
    <span class="s">end</span>

    <span class="c">% Find centers, composite Rho and Delta</span>
    <span class="n">Temp</span> <span class="p">=</span> <span class="n">Rho</span><span class="o">.^</span><span class="mi">2</span> <span class="o">+</span> <span class="n">Delta</span><span class="o">.^</span><span class="mi">2</span><span class="p">;</span>
    <span class="p">[</span><span class="o">~</span><span class="p">,</span> <span class="n">tempIndex</span><span class="p">]</span> <span class="p">=</span> <span class="n">sort</span><span class="p">(</span><span class="n">Temp</span><span class="p">,</span> <span class="s">&#39;descend&#39;</span><span class="p">);</span>
    <span class="c">% highest [NumCategory] values</span>
    <span class="n">tempIndexSlice</span> <span class="p">=</span> <span class="n">tempIndex</span><span class="p">(</span><span class="mi">1</span><span class="p">:</span><span class="n">NumCategory</span><span class="p">);</span>
    <span class="n">centers</span> <span class="p">=</span> <span class="n">dataset</span><span class="p">(</span><span class="n">tempIndexSlice</span><span class="p">,</span> <span class="p">:);</span>

    <span class="c">% Return</span>
    <span class="n">Rho_o</span> <span class="p">=</span> <span class="n">Rho</span><span class="p">;</span>
    <span class="n">Delta_o</span> <span class="p">=</span> <span class="n">Delta</span><span class="p">;</span>
    <span class="n">centers_o</span> <span class="p">=</span> <span class="n">centers</span><span class="p">;</span>
<span class="n">end</span>
</pre></div>
<p>You can see the core part contains less than 50 lines of code, it is so concise that I even doubted myself. But the result reassured me：</p>
<p><figure style="flex: 117.54756871035941" ><img width="1112" height="473" src="https://cdn.jsdelivr.net/gh/AlanDecode/site-Blog@gh-pages/archives/assets/91e42a37259c862eff50b74848c93363.jpg" /><figcaption>Result of FSFDP algorithm</figcaption></figure></p>
<p>Cluster centers are drawn using blue circles on the right-hand figure. And we can clearly see 15 points with high $\rho$ and $\delta$ one the left-hand figure. This algorithm is not iterative, the most time consuming step is sorting. This is inevitable though, because the sorting result is necessary for calculating $\delta$.</p>
<hr>
<p>Dataset in this post could be downloaded <a href="/resources/s1.txt">here</a>, just load it in Matlab. It includes coordinates of 5000 2-D points.</p>
<hr><div class="footnotes"><ol><li id="fn_1">the differences of data points in the same class <a no-style href="#fn_ref_1">↩</a></li><li id="fn_2">the differences of data points in different classes <a no-style href="#fn_ref_2">↩</a></li></ol></div>
            </div>
        </article>
        <div id="ga-tags">
    
    <span class="ga-tag">
        <a class="ga-highlight" href="/tag/算法/">#算法</a>
    </span>
    
    <span class="ga-tag">
        <a class="ga-highlight" href="/tag/机器学习/">#机器学习</a>
    </span>
    
</div>
    </section>

    
<section id="ga-content_pager">

    <div class="next">
        <a class="ga-highlight" href="/archives/321/">狂人日记</a>
        <p class="yue">我横竖睡不着，仔细看了半夜，才从字缝里看出字来，满本都写着两个字是“吃人”！</p>
    </div>


    <div class="prev">
        <a class="ga-highlight" href="/archives/317/">春天、联谊、新耳机</a>
        <p class="yue">趁着联谊记忆还在，新耳机的新鲜感尚未消退，还是赶紧把当前的心绪记录下来。</p>
    </div>

</section>


    
        <script>
            var initValine = function () {
                new Valine({"enable": true, "el": "#vcomments", "appId": "6chFXPTjrjYnjFk9duROcboN-gzGzoHsz", "appKey": "c1CRooaFmpLs4xi7x3YLm3ma", "visitor": true, "recordIP": true, "placeholder": "\u6765\u7545\u6240\u6b32\u8a00\u5427~"});
            }
        </script>
        <script defer src='https://cdn.jsdelivr.net/npm/valine@1.3.10/dist/Valine.min.js' onload="initValine()"></script>
        <div id="vcomments"></div>
    

</main>

                <footer class="ga-mono" id="ga-footer">
                    <section>
                        <span id="ga-uptime"></span>
                        <span class="brand">智的博客</span>
                    </section>
                    <section>
                        <p class="copyright">
                            <span>Copyright © 2020 熊猫小A</span>
                            <span>Powered by <a no-style href="https://github.com/AlanDecode/Maverick" target="_blank">Maverick & Galileo</a></span>
                        </p>
                        <div class="copyright">
                            <span class="footer-addon">
                                
<a no-style href="http://beian.miit.gov.cn" target="_blank">京ICP备18000133号-1</a> | 
<a no-style href="https://www.upyun.com" target="_blank">又拍云</a>

                            </span>
                            <nav class="social-links">
                                <ul><li><a class="no-style" title="Twitter" href="https://twitter.com/AlanDecode" target="_blank"><i class="gi gi-twitter"></i>Twitter</a></li><span class="separator">·</span><li><a class="no-style" title="GitHub" href="https://github.com/AlanDecode" target="_blank"><i class="gi gi-github"></i>GitHub</a></li><span class="separator">·</span><li><a class="no-style" title="Weibo" href="https://weibo.com/5245109677/" target="_blank"><i class="gi gi-weibo"></i>Weibo</a></li></ul>
                            </nav>
                        </div>
                    </section>
                    <script>
                        var site_build_date = "2017-06-29T12:00+08:00"
                    </script>
                    <script src="https://cdn.jsdelivr.net/gh/AlanDecode/site-Blog@gh-pages/assets/galileo-dc4baa7cf4.js"></script>
                </footer>
            </div>
        </div>
    </div>

    <!--katex-->
    <script defer src="https://cdn.jsdelivr.net/gh/AlanDecode/site-Blog@gh-pages/assets/katex.min.js"></script>
    <script>
    mathOpts = {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "\\[", right: "\\]", display: true},
            {left: "$", right: "$", display: false},
            {left: "\\(", right: "\\)", display: false}
        ]
    };
    </script>
    <script defer src="https://cdn.jsdelivr.net/gh/AlanDecode/site-Blog@gh-pages/assets/auto-render.min.js" onload="renderMathInElement(document.body, mathOpts);"></script>

    <script src="https://cdn.jsdelivr.net/gh/AlanDecode/site-Blog@gh-pages/assets/ExSearch/jquery.min.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/AlanDecode/site-Blog@gh-pages/assets/ExSearch/ExSearch-493cb9cd89.js"></script>

    
<script>
    var _hmt = _hmt || [];
    (function() {
    var hm = document.createElement("script");
    hm.src = "https://hm.baidu.com/hm.js?e4f3a7c02ac2aabc41a1cfa95f61a026";
    var s = document.getElementsByTagName("script")[0]; 
    s.parentNode.insertBefore(hm, s);
    })();
</script>
<script>
    (function(){
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        }
        else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>
<script>
if(window.location.hash){
    var checkExist = setInterval(function() {
       if ($(window.location.hash).length) {
          $('html, body').animate({scrollTop: $(window.location.hash).offset().top-90}, 1000);
          clearInterval(checkExist);
       }
    }, 100);
}
</script>
<script>
if(window.navigator && navigator.serviceWorker) {
  caches.keys().then(function(cacheNames) {
    cacheNames.forEach(function(cacheName) {
      caches.delete(cacheName);
    });
  }).then(function(){
    console.log('Cache cleaned.');
  });
  navigator.serviceWorker.getRegistrations()
  .then(function(registrations) {
    for(let registration of registrations) {
      registration.unregister();
    }
  }).then(function(){
    console.log('Service Worker stopped.');
  });
}
</script>

    </body>
</html>